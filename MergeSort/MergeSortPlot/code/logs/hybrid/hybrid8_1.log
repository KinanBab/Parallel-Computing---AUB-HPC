Sender: LSF System <hpcadmin@node08>
Subject: Job 16558: </gpfs1/openmpi/bin/mpirun ./hybrid 8 10000> Done

Job </gpfs1/openmpi/bin/mpirun ./hybrid 8 10000> was submitted from host <head1> by user <kmd14> in cluster <head1_cluster1>.
Job was executed on host(s) <node08>, in queue <medium_priority>, as user <kmd14> in cluster <head1_cluster1>.
</gpfs1/kmd14> was used as the home directory.
</gpfs1/kmd14/297M/Assignments/ExtraGrades/mergesort> was used as the working directory.
Started at Wed Apr 16 02:38:55 2014
Results reported at Wed Apr 16 02:39:00 2014

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/gpfs1/openmpi/bin/mpirun ./hybrid 8 10000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time   :      1.02 sec.
    Max Memory :         1 MB
    Max Swap   :        30 MB

    Max Processes  :         1
    Max Threads    :         1

The output (if any) follows:

--------------------------------------------------------------------------
WARNING: It appears that your OpenFabrics subsystem is configured to only
allow registering part of your physical memory.  This can cause MPI jobs to
run with erratic performance, hang, and/or crash.

This may be caused by your OpenFabrics vendor limiting the amount of
physical memory that can be registered.  You should investigate the
relevant Linux kernel module parameters that control how much physical
memory can be registered, and increase them to allow registering all
physical memory on your machine.

See this Open MPI FAQ item for more information on these Linux kernel module
parameters:

    http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages

  Local host:              node08
  Registerable memory:     32768 MiB
  Total memory:            65508 MiB

Your MPI job will continue, but may be behave poorly and/or hang.
--------------------------------------------------------------------------
[Threads] Nodes: 1 Threads/Node: 8
[Realtime]	 0.094446
[Clocktime]	 0.550000
Sender: LSF System <hpcadmin@node08>
Subject: Job 16559: </gpfs1/openmpi/bin/mpirun ./hybrid 8 1000000> Done

Job </gpfs1/openmpi/bin/mpirun ./hybrid 8 1000000> was submitted from host <head1> by user <kmd14> in cluster <head1_cluster1>.
Job was executed on host(s) <node08>, in queue <medium_priority>, as user <kmd14> in cluster <head1_cluster1>.
</gpfs1/kmd14> was used as the home directory.
</gpfs1/kmd14/297M/Assignments/ExtraGrades/mergesort> was used as the working directory.
Started at Wed Apr 16 02:38:55 2014
Results reported at Wed Apr 16 02:39:00 2014

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/gpfs1/openmpi/bin/mpirun ./hybrid 8 1000000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time   :      1.59 sec.
    Max Memory :         1 MB
    Max Swap   :        30 MB

    Max Processes  :         1
    Max Threads    :         1

The output (if any) follows:

--------------------------------------------------------------------------
WARNING: It appears that your OpenFabrics subsystem is configured to only
allow registering part of your physical memory.  This can cause MPI jobs to
run with erratic performance, hang, and/or crash.

This may be caused by your OpenFabrics vendor limiting the amount of
physical memory that can be registered.  You should investigate the
relevant Linux kernel module parameters that control how much physical
memory can be registered, and increase them to allow registering all
physical memory on your machine.

See this Open MPI FAQ item for more information on these Linux kernel module
parameters:

    http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages

  Local host:              node08
  Registerable memory:     32768 MiB
  Total memory:            65508 MiB

Your MPI job will continue, but may be behave poorly and/or hang.
--------------------------------------------------------------------------
[Threads] Nodes: 1 Threads/Node: 8
[Realtime]	 0.323781
[Clocktime]	 1.150000
Sender: LSF System <hpcadmin@node08>
Subject: Job 16560: </gpfs1/openmpi/bin/mpirun ./hybrid 8 100000000> Done

Job </gpfs1/openmpi/bin/mpirun ./hybrid 8 100000000> was submitted from host <head1> by user <kmd14> in cluster <head1_cluster1>.
Job was executed on host(s) <node08>, in queue <medium_priority>, as user <kmd14> in cluster <head1_cluster1>.
</gpfs1/kmd14> was used as the home directory.
</gpfs1/kmd14/297M/Assignments/ExtraGrades/mergesort> was used as the working directory.
Started at Wed Apr 16 02:38:55 2014
Results reported at Wed Apr 16 02:39:24 2014

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/gpfs1/openmpi/bin/mpirun ./hybrid 8 100000000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time   :     67.17 sec.
    Max Memory :       774 MB
    Max Swap   :      1152 MB

    Max Processes  :         4
    Max Threads    :        56

The output (if any) follows:

--------------------------------------------------------------------------
WARNING: It appears that your OpenFabrics subsystem is configured to only
allow registering part of your physical memory.  This can cause MPI jobs to
run with erratic performance, hang, and/or crash.

This may be caused by your OpenFabrics vendor limiting the amount of
physical memory that can be registered.  You should investigate the
relevant Linux kernel module parameters that control how much physical
memory can be registered, and increase them to allow registering all
physical memory on your machine.

See this Open MPI FAQ item for more information on these Linux kernel module
parameters:

    http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages

  Local host:              node08
  Registerable memory:     32768 MiB
  Total memory:            65508 MiB

Your MPI job will continue, but may be behave poorly and/or hang.
--------------------------------------------------------------------------
[Threads] Nodes: 1 Threads/Node: 8
[Realtime]	 16.872104
[Clocktime]	 64.850000
Sender: LSF System <hpcadmin@node08>
Subject: Job 16561: </gpfs1/openmpi/bin/mpirun ./hybrid 8 200000000> Done

Job </gpfs1/openmpi/bin/mpirun ./hybrid 8 200000000> was submitted from host <head1> by user <kmd14> in cluster <head1_cluster1>.
Job was executed on host(s) <node08>, in queue <medium_priority>, as user <kmd14> in cluster <head1_cluster1>.
</gpfs1/kmd14> was used as the home directory.
</gpfs1/kmd14/297M/Assignments/ExtraGrades/mergesort> was used as the working directory.
Started at Wed Apr 16 02:38:55 2014
Results reported at Wed Apr 16 02:39:45 2014

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/gpfs1/openmpi/bin/mpirun ./hybrid 8 200000000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time   :    135.41 sec.
    Max Memory :      1300 MB
    Max Swap   :      1914 MB

    Max Processes  :         4
    Max Threads    :        56

The output (if any) follows:

--------------------------------------------------------------------------
WARNING: It appears that your OpenFabrics subsystem is configured to only
allow registering part of your physical memory.  This can cause MPI jobs to
run with erratic performance, hang, and/or crash.

This may be caused by your OpenFabrics vendor limiting the amount of
physical memory that can be registered.  You should investigate the
relevant Linux kernel module parameters that control how much physical
memory can be registered, and increase them to allow registering all
physical memory on your machine.

See this Open MPI FAQ item for more information on these Linux kernel module
parameters:

    http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages

  Local host:              node08
  Registerable memory:     32768 MiB
  Total memory:            65508 MiB

Your MPI job will continue, but may be behave poorly and/or hang.
--------------------------------------------------------------------------
[Threads] Nodes: 1 Threads/Node: 8
[Realtime]	 39.147273
[Clocktime]	 130.730000
Sender: LSF System <hpcadmin@node08>
Subject: Job 16562: </gpfs1/openmpi/bin/mpirun ./hybrid 8 400000000> Done

Job </gpfs1/openmpi/bin/mpirun ./hybrid 8 400000000> was submitted from host <head1> by user <kmd14> in cluster <head1_cluster1>.
Job was executed on host(s) <node08>, in queue <medium_priority>, as user <kmd14> in cluster <head1_cluster1>.
</gpfs1/kmd14> was used as the home directory.
</gpfs1/kmd14/297M/Assignments/ExtraGrades/mergesort> was used as the working directory.
Started at Wed Apr 16 02:39:01 2014
Results reported at Wed Apr 16 02:41:01 2014

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/gpfs1/openmpi/bin/mpirun ./hybrid 8 400000000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time   :    274.58 sec.
    Max Memory :      3067 MB
    Max Swap   :      3440 MB

    Max Processes  :         4
    Max Threads    :        56

The output (if any) follows:

--------------------------------------------------------------------------
WARNING: It appears that your OpenFabrics subsystem is configured to only
allow registering part of your physical memory.  This can cause MPI jobs to
run with erratic performance, hang, and/or crash.

This may be caused by your OpenFabrics vendor limiting the amount of
physical memory that can be registered.  You should investigate the
relevant Linux kernel module parameters that control how much physical
memory can be registered, and increase them to allow registering all
physical memory on your machine.

See this Open MPI FAQ item for more information on these Linux kernel module
parameters:

    http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages

  Local host:              node08
  Registerable memory:     32768 MiB
  Total memory:            65508 MiB

Your MPI job will continue, but may be behave poorly and/or hang.
--------------------------------------------------------------------------
[Threads] Nodes: 1 Threads/Node: 8
[Realtime]	 97.296221
[Clocktime]	 265.150000
Sender: LSF System <hpcadmin@node08>
Subject: Job 16563: </gpfs1/openmpi/bin/mpirun ./hybrid 8 500000000> Done

Job </gpfs1/openmpi/bin/mpirun ./hybrid 8 500000000> was submitted from host <head1> by user <kmd14> in cluster <head1_cluster1>.
Job was executed on host(s) <node08>, in queue <medium_priority>, as user <kmd14> in cluster <head1_cluster1>.
</gpfs1/kmd14> was used as the home directory.
</gpfs1/kmd14/297M/Assignments/ExtraGrades/mergesort> was used as the working directory.
Started at Wed Apr 16 02:39:01 2014
Results reported at Wed Apr 16 02:41:27 2014

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/gpfs1/openmpi/bin/mpirun ./hybrid 8 500000000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time   :    392.24 sec.
    Max Memory :      3826 MB
    Max Swap   :      4203 MB

    Max Processes  :         4
    Max Threads    :        56

The output (if any) follows:

--------------------------------------------------------------------------
WARNING: It appears that your OpenFabrics subsystem is configured to only
allow registering part of your physical memory.  This can cause MPI jobs to
run with erratic performance, hang, and/or crash.

This may be caused by your OpenFabrics vendor limiting the amount of
physical memory that can be registered.  You should investigate the
relevant Linux kernel module parameters that control how much physical
memory can be registered, and increase them to allow registering all
physical memory on your machine.

See this Open MPI FAQ item for more information on these Linux kernel module
parameters:

    http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages

  Local host:              node08
  Registerable memory:     32768 MiB
  Total memory:            65508 MiB

Your MPI job will continue, but may be behave poorly and/or hang.
--------------------------------------------------------------------------
[Threads] Nodes: 1 Threads/Node: 8
[Realtime]	 116.386158
[Clocktime]	 380.440000
