Sender: LSF System <hpcadmin@node04>
Subject: Job 9442: </gpfs1/openmpi/bin/mpirun ./product 100000> Done

Job </gpfs1/openmpi/bin/mpirun ./product 100000> was submitted from host <head1> by user <kmd14> in cluster <head1_cluster1>.
Job was executed on host(s) <16*node04>, in queue <medium_priority>, as user <kmd14> in cluster <head1_cluster1>.
</gpfs1/kmd14> was used as the home directory.
</gpfs1/kmd14/297M/Assignments/Assignment 1/MPI> was used as the working directory.
Started at Mon Feb 24 16:30:54 2014
Results reported at Mon Feb 24 16:30:57 2014

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/gpfs1/openmpi/bin/mpirun ./product 100000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time   :      1.77 sec.
    Max Memory :         1 MB
    Max Swap   :        30 MB

    Max Processes  :         1
    Max Threads    :         1

The output (if any) follows:

--------------------------------------------------------------------------
WARNING: It appears that your OpenFabrics subsystem is configured to only
allow registering part of your physical memory.  This can cause MPI jobs to
run with erratic performance, hang, and/or crash.

This may be caused by your OpenFabrics vendor limiting the amount of
physical memory that can be registered.  You should investigate the
relevant Linux kernel module parameters that control how much physical
memory can be registered, and increase them to allow registering all
physical memory on your machine.

See this Open MPI FAQ item for more information on these Linux kernel module
parameters:

    http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages

  Local host:              node04
  Registerable memory:     32768 MiB
  Total memory:            65508 MiB

Your MPI job will continue, but may be behave poorly and/or hang.
--------------------------------------------------------------------------
Processor 7: sum of squares up to 100000*100000 is 333338333350000
Processor 8: sum of squares up to 100000*100000 is 333338333350000
Processor 9: sum of squares up to 100000*100000 is 333338333350000
Processor 10: sum of squares up to 100000*100000 is 333338333350000
Processor 11: sum of squares up to 100000*100000 is 333338333350000
Processor 12: sum of squares up to 100000*100000 is 333338333350000
Processor 13: sum of squares up to 100000*100000 is 333338333350000
Processor 14: sum of squares up to 100000*100000 is 333338333350000
Processor 15: sum of squares up to 100000*100000 is 333338333350000
Processor 0: sum of squares up to 100000*100000 is 333338333350000
This took only 0.012830 seconds.
Processor 1: sum of squares up to 100000*100000 is 333338333350000
Processor 2: sum of squares up to 100000*100000 is 333338333350000
Processor 3: sum of squares up to 100000*100000 is 333338333350000
Processor 4: sum of squares up to 100000*100000 is 333338333350000
Processor 5: sum of squares up to 100000*100000 is 333338333350000
Processor 6: sum of squares up to 100000*100000 is 333338333350000
[node04:02897] 15 more processes have sent help message help-mpi-btl-openib.txt / reg mem limit low
[node04:02897] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Sender: LSF System <hpcadmin@node16>
Subject: Job 9444: </gpfs1/openmpi/bin/mpirun ./product 200000> Done

Job </gpfs1/openmpi/bin/mpirun ./product 200000> was submitted from host <head1> by user <kmd14> in cluster <head1_cluster1>.
Job was executed on host(s) <16*node16>, in queue <medium_priority>, as user <kmd14> in cluster <head1_cluster1>.
</gpfs1/kmd14> was used as the home directory.
</gpfs1/kmd14/297M/Assignments/Assignment 1/MPI> was used as the working directory.
Started at Mon Feb 24 16:30:54 2014
Results reported at Mon Feb 24 16:30:57 2014

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/gpfs1/openmpi/bin/mpirun ./product 200000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time   :      1.75 sec.
    Max Memory :         1 MB
    Max Swap   :        30 MB

    Max Processes  :         1
    Max Threads    :         1

The output (if any) follows:

--------------------------------------------------------------------------
WARNING: It appears that your OpenFabrics subsystem is configured to only
allow registering part of your physical memory.  This can cause MPI jobs to
run with erratic performance, hang, and/or crash.

This may be caused by your OpenFabrics vendor limiting the amount of
physical memory that can be registered.  You should investigate the
relevant Linux kernel module parameters that control how much physical
memory can be registered, and increase them to allow registering all
physical memory on your machine.

See this Open MPI FAQ item for more information on these Linux kernel module
parameters:

    http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages

  Local host:              node16
  Registerable memory:     32768 MiB
  Total memory:            65508 MiB

Your MPI job will continue, but may be behave poorly and/or hang.
--------------------------------------------------------------------------
Processor 8: sum of squares up to 200000*200000 is 2666686666700000
Processor 10: sum of squares up to 200000*200000 is 2666686666700000
Processor 12: sum of squares up to 200000*200000 is 2666686666700000
Processor 14: sum of squares up to 200000*200000 is 2666686666700000
Processor 0: sum of squares up to 200000*200000 is 2666686666700000
This took only 0.005967 seconds.
Processor 2: sum of squares up to 200000*200000 is 2666686666700000
Processor 4: sum of squares up to 200000*200000 is 2666686666700000
Processor 6: sum of squares up to 200000*200000 is 2666686666700000
Processor 1: sum of squares up to 200000*200000 is 2666686666700000
Processor 3: sum of squares up to 200000*200000 is 2666686666700000
Processor 5: sum of squares up to 200000*200000 is 2666686666700000
Processor 7: sum of squares up to 200000*200000 is 2666686666700000
Processor 9: sum of squares up to 200000*200000 is 2666686666700000
Processor 11: sum of squares up to 200000*200000 is 2666686666700000
Processor 13: sum of squares up to 200000*200000 is 2666686666700000
Processor 15: sum of squares up to 200000*200000 is 2666686666700000
[node16:12860] 15 more processes have sent help message help-mpi-btl-openib.txt / reg mem limit low
[node16:12860] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Sender: LSF System <hpcadmin@node01>
Subject: Job 9445: </gpfs1/openmpi/bin/mpirun ./product 250000> Done

Job </gpfs1/openmpi/bin/mpirun ./product 250000> was submitted from host <head1> by user <kmd14> in cluster <head1_cluster1>.
Job was executed on host(s) <16*node01>, in queue <medium_priority>, as user <kmd14> in cluster <head1_cluster1>.
</gpfs1/kmd14> was used as the home directory.
</gpfs1/kmd14/297M/Assignments/Assignment 1/MPI> was used as the working directory.
Started at Mon Feb 24 16:30:56 2014
Results reported at Mon Feb 24 16:30:57 2014

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/gpfs1/openmpi/bin/mpirun ./product 250000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time   :      1.78 sec.
    Max Memory :         1 MB
    Max Swap   :        30 MB

    Max Processes  :         1
    Max Threads    :         1

The output (if any) follows:

--------------------------------------------------------------------------
WARNING: It appears that your OpenFabrics subsystem is configured to only
allow registering part of your physical memory.  This can cause MPI jobs to
run with erratic performance, hang, and/or crash.

This may be caused by your OpenFabrics vendor limiting the amount of
physical memory that can be registered.  You should investigate the
relevant Linux kernel module parameters that control how much physical
memory can be registered, and increase them to allow registering all
physical memory on your machine.

See this Open MPI FAQ item for more information on these Linux kernel module
parameters:

    http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages

  Local host:              node01
  Registerable memory:     32768 MiB
  Total memory:            65508 MiB

Your MPI job will continue, but may be behave poorly and/or hang.
--------------------------------------------------------------------------
Processor 12: sum of squares up to 250000*250000 is 5208364583375000
Processor 13: sum of squares up to 250000*250000 is 5208364583375000
Processor 14: sum of squares up to 250000*250000 is 5208364583375000
Processor 15: sum of squares up to 250000*250000 is 5208364583375000
Processor 0: sum of squares up to 250000*250000 is 5208364583375000
This took only 0.017553 seconds.
Processor 1: sum of squares up to 250000*250000 is 5208364583375000
Processor 2: sum of squares up to 250000*250000 is 5208364583375000
Processor 3: sum of squares up to 250000*250000 is 5208364583375000
Processor 4: sum of squares up to 250000*250000 is 5208364583375000
Processor 5: sum of squares up to 250000*250000 is 5208364583375000
Processor 6: sum of squares up to 250000*250000 is 5208364583375000
Processor 7: sum of squares up to 250000*250000 is 5208364583375000
Processor 8: sum of squares up to 250000*250000 is 5208364583375000
Processor 9: sum of squares up to 250000*250000 is 5208364583375000
Processor 10: sum of squares up to 250000*250000 is 5208364583375000
Processor 11: sum of squares up to 250000*250000 is 5208364583375000
[node01:22874] 15 more processes have sent help message help-mpi-btl-openib.txt / reg mem limit low
[node01:22874] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Sender: LSF System <hpcadmin@node06>
Subject: Job 9443: </gpfs1/openmpi/bin/mpirun ./product 150000> Done

Job </gpfs1/openmpi/bin/mpirun ./product 150000> was submitted from host <head1> by user <kmd14> in cluster <head1_cluster1>.
Job was executed on host(s) <16*node06>, in queue <medium_priority>, as user <kmd14> in cluster <head1_cluster1>.
</gpfs1/kmd14> was used as the home directory.
</gpfs1/kmd14/297M/Assignments/Assignment 1/MPI> was used as the working directory.
Started at Mon Feb 24 16:30:54 2014
Results reported at Mon Feb 24 16:30:58 2014

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/gpfs1/openmpi/bin/mpirun ./product 150000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time   :      1.99 sec.
    Max Memory :         1 MB
    Max Swap   :        30 MB

    Max Processes  :         1
    Max Threads    :         1

The output (if any) follows:

--------------------------------------------------------------------------
WARNING: It appears that your OpenFabrics subsystem is configured to only
allow registering part of your physical memory.  This can cause MPI jobs to
run with erratic performance, hang, and/or crash.

This may be caused by your OpenFabrics vendor limiting the amount of
physical memory that can be registered.  You should investigate the
relevant Linux kernel module parameters that control how much physical
memory can be registered, and increase them to allow registering all
physical memory on your machine.

See this Open MPI FAQ item for more information on these Linux kernel module
parameters:

    http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages

  Local host:              node06
  Registerable memory:     32768 MiB
  Total memory:            65508 MiB

Your MPI job will continue, but may be behave poorly and/or hang.
--------------------------------------------------------------------------
Processor 5: sum of squares up to 150000*150000 is 1125011250025000
Processor 7: sum of squares up to 150000*150000 is 1125011250025000
Processor 9: sum of squares up to 150000*150000 is 1125011250025000
Processor 11: sum of squares up to 150000*150000 is 1125011250025000
Processor 12: sum of squares up to 150000*150000 is 1125011250025000
Processor 13: sum of squares up to 150000*150000 is 1125011250025000
Processor 14: sum of squares up to 150000*150000 is 1125011250025000
Processor 15: sum of squares up to 150000*150000 is 1125011250025000
Processor 1: sum of squares up to 150000*150000 is 1125011250025000
Processor 3: sum of squares up to 150000*150000 is 1125011250025000
Processor 10: sum of squares up to 150000*150000 is 1125011250025000
Processor 0: sum of squares up to 150000*150000 is 1125011250025000
This took only 0.021297 seconds.
Processor 2: sum of squares up to 150000*150000 is 1125011250025000
Processor 4: sum of squares up to 150000*150000 is 1125011250025000
Processor 6: sum of squares up to 150000*150000 is 1125011250025000
Processor 8: sum of squares up to 150000*150000 is 1125011250025000
[node06:28455] 15 more processes have sent help message help-mpi-btl-openib.txt / reg mem limit low
[node06:28455] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Sender: LSF System <hpcadmin@node13>
Subject: Job 9446: </gpfs1/openmpi/bin/mpirun ./product 300000> Done

Job </gpfs1/openmpi/bin/mpirun ./product 300000> was submitted from host <head1> by user <kmd14> in cluster <head1_cluster1>.
Job was executed on host(s) <16*node13>, in queue <medium_priority>, as user <kmd14> in cluster <head1_cluster1>.
</gpfs1/kmd14> was used as the home directory.
</gpfs1/kmd14/297M/Assignments/Assignment 1/MPI> was used as the working directory.
Started at Mon Feb 24 16:30:57 2014
Results reported at Mon Feb 24 16:30:59 2014

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/gpfs1/openmpi/bin/mpirun ./product 300000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time   :      1.79 sec.
    Max Memory :         1 MB
    Max Swap   :        30 MB

    Max Processes  :         1
    Max Threads    :         1

The output (if any) follows:

--------------------------------------------------------------------------
WARNING: It appears that your OpenFabrics subsystem is configured to only
allow registering part of your physical memory.  This can cause MPI jobs to
run with erratic performance, hang, and/or crash.

This may be caused by your OpenFabrics vendor limiting the amount of
physical memory that can be registered.  You should investigate the
relevant Linux kernel module parameters that control how much physical
memory can be registered, and increase them to allow registering all
physical memory on your machine.

See this Open MPI FAQ item for more information on these Linux kernel module
parameters:

    http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages

  Local host:              node13
  Registerable memory:     32768 MiB
  Total memory:            65508 MiB

Your MPI job will continue, but may be behave poorly and/or hang.
--------------------------------------------------------------------------
Processor 4: sum of squares up to 300000*300000 is 9000045000050000
Processor 5: sum of squares up to 300000*300000 is 9000045000050000
Processor 6: sum of squares up to 300000*300000 is 9000045000050000
Processor 7: sum of squares up to 300000*300000 is 9000045000050000
Processor 8: sum of squares up to 300000*300000 is 9000045000050000
Processor 9: sum of squares up to 300000*300000 is 9000045000050000
Processor 10: sum of squares up to 300000*300000 is 9000045000050000
Processor 11: sum of squares up to 300000*300000 is 9000045000050000
Processor 12: sum of squares up to 300000*300000 is 9000045000050000
Processor 13: sum of squares up to 300000*300000 is 9000045000050000
Processor 14: sum of squares up to 300000*300000 is 9000045000050000
Processor 15: sum of squares up to 300000*300000 is 9000045000050000
Processor 0: sum of squares up to 300000*300000 is 9000045000050000
This took only 0.006070 seconds.
Processor 1: sum of squares up to 300000*300000 is 9000045000050000
Processor 2: sum of squares up to 300000*300000 is 9000045000050000
Processor 3: sum of squares up to 300000*300000 is 9000045000050000
[node13:28977] 15 more processes have sent help message help-mpi-btl-openib.txt / reg mem limit low
[node13:28977] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Sender: LSF System <hpcadmin@node03>
Subject: Job 9447: </gpfs1/openmpi/bin/mpirun ./product 350000> Done

Job </gpfs1/openmpi/bin/mpirun ./product 350000> was submitted from host <head1> by user <kmd14> in cluster <head1_cluster1>.
Job was executed on host(s) <16*node03>, in queue <medium_priority>, as user <kmd14> in cluster <head1_cluster1>.
</gpfs1/kmd14> was used as the home directory.
</gpfs1/kmd14/297M/Assignments/Assignment 1/MPI> was used as the working directory.
Started at Mon Feb 24 16:30:58 2014
Results reported at Mon Feb 24 16:31:00 2014

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/gpfs1/openmpi/bin/mpirun ./product 350000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time   :      1.66 sec.
    Max Memory :         1 MB
    Max Swap   :        30 MB

    Max Processes  :         1
    Max Threads    :         1

The output (if any) follows:

--------------------------------------------------------------------------
WARNING: It appears that your OpenFabrics subsystem is configured to only
allow registering part of your physical memory.  This can cause MPI jobs to
run with erratic performance, hang, and/or crash.

This may be caused by your OpenFabrics vendor limiting the amount of
physical memory that can be registered.  You should investigate the
relevant Linux kernel module parameters that control how much physical
memory can be registered, and increase them to allow registering all
physical memory on your machine.

See this Open MPI FAQ item for more information on these Linux kernel module
parameters:

    http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages

  Local host:              node03
  Registerable memory:     32768 MiB
  Total memory:            65508 MiB

Your MPI job will continue, but may be behave poorly and/or hang.
--------------------------------------------------------------------------
Processor 0: sum of squares up to 350000*350000 is 14291727916725000
This took only 0.006076 seconds.
Processor 1: sum of squares up to 350000*350000 is 14291727916725000
Processor 2: sum of squares up to 350000*350000 is 14291727916725000
Processor 3: sum of squares up to 350000*350000 is 14291727916725000
Processor 4: sum of squares up to 350000*350000 is 14291727916725000
Processor 5: sum of squares up to 350000*350000 is 14291727916725000
Processor 6: sum of squares up to 350000*350000 is 14291727916725000
Processor 7: sum of squares up to 350000*350000 is 14291727916725000
Processor 8: sum of squares up to 350000*350000 is 14291727916725000
Processor 9: sum of squares up to 350000*350000 is 14291727916725000
Processor 10: sum of squares up to 350000*350000 is 14291727916725000
Processor 11: sum of squares up to 350000*350000 is 14291727916725000
Processor 12: sum of squares up to 350000*350000 is 14291727916725000
Processor 13: sum of squares up to 350000*350000 is 14291727916725000
Processor 14: sum of squares up to 350000*350000 is 14291727916725000
Processor 15: sum of squares up to 350000*350000 is 14291727916725000
[node03:15974] 15 more processes have sent help message help-mpi-btl-openib.txt / reg mem limit low
[node03:15974] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Sender: LSF System <hpcadmin@node04>
Subject: Job 9451: </gpfs1/openmpi/bin/mpirun ./product 550000> Done

Job </gpfs1/openmpi/bin/mpirun ./product 550000> was submitted from host <head1> by user <kmd14> in cluster <head1_cluster1>.
Job was executed on host(s) <16*node04>, in queue <medium_priority>, as user <kmd14> in cluster <head1_cluster1>.
</gpfs1/kmd14> was used as the home directory.
</gpfs1/kmd14/297M/Assignments/Assignment 1/MPI> was used as the working directory.
Started at Mon Feb 24 16:30:58 2014
Results reported at Mon Feb 24 16:31:00 2014

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/gpfs1/openmpi/bin/mpirun ./product 550000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time   :      1.70 sec.
    Max Memory :         1 MB
    Max Swap   :        30 MB

    Max Processes  :         1
    Max Threads    :         1

The output (if any) follows:

--------------------------------------------------------------------------
WARNING: It appears that your OpenFabrics subsystem is configured to only
allow registering part of your physical memory.  This can cause MPI jobs to
run with erratic performance, hang, and/or crash.

This may be caused by your OpenFabrics vendor limiting the amount of
physical memory that can be registered.  You should investigate the
relevant Linux kernel module parameters that control how much physical
memory can be registered, and increase them to allow registering all
physical memory on your machine.

See this Open MPI FAQ item for more information on these Linux kernel module
parameters:

    http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages

  Local host:              node04
  Registerable memory:     32768 MiB
  Total memory:            65508 MiB

Your MPI job will continue, but may be behave poorly and/or hang.
--------------------------------------------------------------------------
Processor 4: sum of squares up to 550000*550000 is 55458484583425000
Processor 5: sum of squares up to 550000*550000 is 55458484583425000
Processor 6: sum of squares up to 550000*550000 is 55458484583425000
Processor 7: sum of squares up to 550000*550000 is 55458484583425000
Processor 8: sum of squares up to 550000*550000 is 55458484583425000
Processor 9: sum of squares up to 550000*550000 is 55458484583425000
Processor 10: sum of squares up to 550000*550000 is 55458484583425000
Processor 11: sum of squares up to 550000*550000 is 55458484583425000
Processor 12: sum of squares up to 550000*550000 is 55458484583425000
Processor 13: sum of squares up to 550000*550000 is 55458484583425000
Processor 14: sum of squares up to 550000*550000 is 55458484583425000
Processor 15: sum of squares up to 550000*550000 is 55458484583425000
Processor 0: sum of squares up to 550000*550000 is 55458484583425000
This took only 0.006452 seconds.
Processor 1: sum of squares up to 550000*550000 is 55458484583425000
Processor 2: sum of squares up to 550000*550000 is 55458484583425000
Processor 3: sum of squares up to 550000*550000 is 55458484583425000
[node04:02951] 15 more processes have sent help message help-mpi-btl-openib.txt / reg mem limit low
[node04:02951] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Sender: LSF System <hpcadmin@node16>
Subject: Job 9449: </gpfs1/openmpi/bin/mpirun ./product 450000> Done

Job </gpfs1/openmpi/bin/mpirun ./product 450000> was submitted from host <head1> by user <kmd14> in cluster <head1_cluster1>.
Job was executed on host(s) <16*node16>, in queue <medium_priority>, as user <kmd14> in cluster <head1_cluster1>.
</gpfs1/kmd14> was used as the home directory.
</gpfs1/kmd14/297M/Assignments/Assignment 1/MPI> was used as the working directory.
Started at Mon Feb 24 16:30:58 2014
Results reported at Mon Feb 24 16:31:00 2014

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/gpfs1/openmpi/bin/mpirun ./product 450000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time   :      1.74 sec.
    Max Memory :         1 MB
    Max Swap   :        30 MB

    Max Processes  :         1
    Max Threads    :         1

The output (if any) follows:

--------------------------------------------------------------------------
WARNING: It appears that your OpenFabrics subsystem is configured to only
allow registering part of your physical memory.  This can cause MPI jobs to
run with erratic performance, hang, and/or crash.

This may be caused by your OpenFabrics vendor limiting the amount of
physical memory that can be registered.  You should investigate the
relevant Linux kernel module parameters that control how much physical
memory can be registered, and increase them to allow registering all
physical memory on your machine.

See this Open MPI FAQ item for more information on these Linux kernel module
parameters:

    http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages

  Local host:              node16
  Registerable memory:     32768 MiB
  Total memory:            65508 MiB

Your MPI job will continue, but may be behave poorly and/or hang.
--------------------------------------------------------------------------
Processor 0: sum of squares up to 450000*450000 is 30375101250075000
This took only 0.006699 seconds.
Processor 1: sum of squares up to 450000*450000 is 30375101250075000
Processor 2: sum of squares up to 450000*450000 is 30375101250075000
Processor 3: sum of squares up to 450000*450000 is 30375101250075000
Processor 4: sum of squares up to 450000*450000 is 30375101250075000
Processor 5: sum of squares up to 450000*450000 is 30375101250075000
Processor 6: sum of squares up to 450000*450000 is 30375101250075000
Processor 7: sum of squares up to 450000*450000 is 30375101250075000
Processor 8: sum of squares up to 450000*450000 is 30375101250075000
Processor 9: sum of squares up to 450000*450000 is 30375101250075000
Processor 10: sum of squares up to 450000*450000 is 30375101250075000
Processor 11: sum of squares up to 450000*450000 is 30375101250075000
Processor 12: sum of squares up to 450000*450000 is 30375101250075000
Processor 13: sum of squares up to 450000*450000 is 30375101250075000
Processor 14: sum of squares up to 450000*450000 is 30375101250075000
Processor 15: sum of squares up to 450000*450000 is 30375101250075000
[node16:12922] 15 more processes have sent help message help-mpi-btl-openib.txt / reg mem limit low
[node16:12922] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Sender: LSF System <hpcadmin@node06>
Subject: Job 9448: </gpfs1/openmpi/bin/mpirun ./product 400000> Done

Job </gpfs1/openmpi/bin/mpirun ./product 400000> was submitted from host <head1> by user <kmd14> in cluster <head1_cluster1>.
Job was executed on host(s) <16*node06>, in queue <medium_priority>, as user <kmd14> in cluster <head1_cluster1>.
</gpfs1/kmd14> was used as the home directory.
</gpfs1/kmd14/297M/Assignments/Assignment 1/MPI> was used as the working directory.
Started at Mon Feb 24 16:30:58 2014
Results reported at Mon Feb 24 16:31:01 2014

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/gpfs1/openmpi/bin/mpirun ./product 400000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time   :      1.65 sec.
    Max Memory :         1 MB
    Max Swap   :        30 MB

    Max Processes  :         1
    Max Threads    :         1

The output (if any) follows:

--------------------------------------------------------------------------
WARNING: It appears that your OpenFabrics subsystem is configured to only
allow registering part of your physical memory.  This can cause MPI jobs to
run with erratic performance, hang, and/or crash.

This may be caused by your OpenFabrics vendor limiting the amount of
physical memory that can be registered.  You should investigate the
relevant Linux kernel module parameters that control how much physical
memory can be registered, and increase them to allow registering all
physical memory on your machine.

See this Open MPI FAQ item for more information on these Linux kernel module
parameters:

    http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages

  Local host:              node06
  Registerable memory:     32768 MiB
  Total memory:            65508 MiB

Your MPI job will continue, but may be behave poorly and/or hang.
--------------------------------------------------------------------------
Processor 7: sum of squares up to 400000*400000 is 21333413333400000
Processor 8: sum of squares up to 400000*400000 is 21333413333400000
Processor 9: sum of squares up to 400000*400000 is 21333413333400000
Processor 10: sum of squares up to 400000*400000 is 21333413333400000
Processor 11: sum of squares up to 400000*400000 is 21333413333400000
Processor 12: sum of squares up to 400000*400000 is 21333413333400000
Processor 13: sum of squares up to 400000*400000 is 21333413333400000
Processor 14: sum of squares up to 400000*400000 is 21333413333400000
Processor 15: sum of squares up to 400000*400000 is 21333413333400000
Processor 0: sum of squares up to 400000*400000 is 21333413333400000
This took only 0.004180 seconds.
Processor 1: sum of squares up to 400000*400000 is 21333413333400000
Processor 2: sum of squares up to 400000*400000 is 21333413333400000
Processor 3: sum of squares up to 400000*400000 is 21333413333400000
Processor 4: sum of squares up to 400000*400000 is 21333413333400000
Processor 5: sum of squares up to 400000*400000 is 21333413333400000
Processor 6: sum of squares up to 400000*400000 is 21333413333400000
[node06:28521] 15 more processes have sent help message help-mpi-btl-openib.txt / reg mem limit low
[node06:28521] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Sender: LSF System <hpcadmin@node01>
Subject: Job 9450: </gpfs1/openmpi/bin/mpirun ./product 500000> Done

Job </gpfs1/openmpi/bin/mpirun ./product 500000> was submitted from host <head1> by user <kmd14> in cluster <head1_cluster1>.
Job was executed on host(s) <16*node01>, in queue <medium_priority>, as user <kmd14> in cluster <head1_cluster1>.
</gpfs1/kmd14> was used as the home directory.
</gpfs1/kmd14/297M/Assignments/Assignment 1/MPI> was used as the working directory.
Started at Mon Feb 24 16:30:58 2014
Results reported at Mon Feb 24 16:31:01 2014

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/gpfs1/openmpi/bin/mpirun ./product 500000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time   :      1.87 sec.
    Max Memory :         1 MB
    Max Swap   :        30 MB

    Max Processes  :         1
    Max Threads    :         1

The output (if any) follows:

--------------------------------------------------------------------------
WARNING: It appears that your OpenFabrics subsystem is configured to only
allow registering part of your physical memory.  This can cause MPI jobs to
run with erratic performance, hang, and/or crash.

This may be caused by your OpenFabrics vendor limiting the amount of
physical memory that can be registered.  You should investigate the
relevant Linux kernel module parameters that control how much physical
memory can be registered, and increase them to allow registering all
physical memory on your machine.

See this Open MPI FAQ item for more information on these Linux kernel module
parameters:

    http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages

  Local host:              node01
  Registerable memory:     32768 MiB
  Total memory:            65508 MiB

Your MPI job will continue, but may be behave poorly and/or hang.
--------------------------------------------------------------------------
Processor 4: sum of squares up to 500000*500000 is 41666791666750000
Processor 5: sum of squares up to 500000*500000 is 41666791666750000
Processor 6: sum of squares up to 500000*500000 is 41666791666750000
Processor 7: sum of squares up to 500000*500000 is 41666791666750000
Processor 8: sum of squares up to 500000*500000 is 41666791666750000
Processor 9: sum of squares up to 500000*500000 is 41666791666750000
Processor 10: sum of squares up to 500000*500000 is 41666791666750000
Processor 11: sum of squares up to 500000*500000 is 41666791666750000
Processor 12: sum of squares up to 500000*500000 is 41666791666750000
Processor 13: sum of squares up to 500000*500000 is 41666791666750000
Processor 14: sum of squares up to 500000*500000 is 41666791666750000
Processor 15: sum of squares up to 500000*500000 is 41666791666750000
Processor 1: sum of squares up to 500000*500000 is 41666791666750000
Processor 2: sum of squares up to 500000*500000 is 41666791666750000
Processor 3: sum of squares up to 500000*500000 is 41666791666750000
Processor 0: sum of squares up to 500000*500000 is 41666791666750000
This took only 0.019942 seconds.
[node01:22928] 15 more processes have sent help message help-mpi-btl-openib.txt / reg mem limit low
[node01:22928] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Sender: LSF System <hpcadmin@node13>
Subject: Job 9452: </gpfs1/openmpi/bin/mpirun ./product 600000> Done

Job </gpfs1/openmpi/bin/mpirun ./product 600000> was submitted from host <head1> by user <kmd14> in cluster <head1_cluster1>.
Job was executed on host(s) <16*node13>, in queue <medium_priority>, as user <kmd14> in cluster <head1_cluster1>.
</gpfs1/kmd14> was used as the home directory.
</gpfs1/kmd14/297M/Assignments/Assignment 1/MPI> was used as the working directory.
Started at Mon Feb 24 16:30:59 2014
Results reported at Mon Feb 24 16:31:01 2014

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/gpfs1/openmpi/bin/mpirun ./product 600000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time   :      1.63 sec.
    Max Memory :         1 MB
    Max Swap   :        30 MB

    Max Processes  :         1
    Max Threads    :         1

The output (if any) follows:

--------------------------------------------------------------------------
WARNING: It appears that your OpenFabrics subsystem is configured to only
allow registering part of your physical memory.  This can cause MPI jobs to
run with erratic performance, hang, and/or crash.

This may be caused by your OpenFabrics vendor limiting the amount of
physical memory that can be registered.  You should investigate the
relevant Linux kernel module parameters that control how much physical
memory can be registered, and increase them to allow registering all
physical memory on your machine.

See this Open MPI FAQ item for more information on these Linux kernel module
parameters:

    http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages

  Local host:              node13
  Registerable memory:     32768 MiB
  Total memory:            65508 MiB

Your MPI job will continue, but may be behave poorly and/or hang.
--------------------------------------------------------------------------
Processor 0: sum of squares up to 600000*600000 is 72000180000100000
This took only 0.002258 seconds.
Processor 1: sum of squares up to 600000*600000 is 72000180000100000
Processor 2: sum of squares up to 600000*600000 is 72000180000100000
Processor 3: sum of squares up to 600000*600000 is 72000180000100000
Processor 4: sum of squares up to 600000*600000 is 72000180000100000
Processor 5: sum of squares up to 600000*600000 is 72000180000100000
Processor 6: sum of squares up to 600000*600000 is 72000180000100000
Processor 7: sum of squares up to 600000*600000 is 72000180000100000
Processor 8: sum of squares up to 600000*600000 is 72000180000100000
Processor 9: sum of squares up to 600000*600000 is 72000180000100000
Processor 10: sum of squares up to 600000*600000 is 72000180000100000
Processor 11: sum of squares up to 600000*600000 is 72000180000100000
Processor 12: sum of squares up to 600000*600000 is 72000180000100000
Processor 13: sum of squares up to 600000*600000 is 72000180000100000
Processor 14: sum of squares up to 600000*600000 is 72000180000100000
Processor 15: sum of squares up to 600000*600000 is 72000180000100000
[node13:29031] 15 more processes have sent help message help-mpi-btl-openib.txt / reg mem limit low
[node13:29031] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Sender: LSF System <hpcadmin@node03>
Subject: Job 9453: </gpfs1/openmpi/bin/mpirun ./product 650000> Done

Job </gpfs1/openmpi/bin/mpirun ./product 650000> was submitted from host <head1> by user <kmd14> in cluster <head1_cluster1>.
Job was executed on host(s) <16*node03>, in queue <medium_priority>, as user <kmd14> in cluster <head1_cluster1>.
</gpfs1/kmd14> was used as the home directory.
</gpfs1/kmd14/297M/Assignments/Assignment 1/MPI> was used as the working directory.
Started at Mon Feb 24 16:31:01 2014
Results reported at Mon Feb 24 16:31:03 2014

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/gpfs1/openmpi/bin/mpirun ./product 650000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time   :      1.81 sec.
    Max Memory :         1 MB
    Max Swap   :        30 MB

    Max Processes  :         1
    Max Threads    :         1

The output (if any) follows:

--------------------------------------------------------------------------
WARNING: It appears that your OpenFabrics subsystem is configured to only
allow registering part of your physical memory.  This can cause MPI jobs to
run with erratic performance, hang, and/or crash.

This may be caused by your OpenFabrics vendor limiting the amount of
physical memory that can be registered.  You should investigate the
relevant Linux kernel module parameters that control how much physical
memory can be registered, and increase them to allow registering all
physical memory on your machine.

See this Open MPI FAQ item for more information on these Linux kernel module
parameters:

    http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages

  Local host:              node03
  Registerable memory:     32768 MiB
  Total memory:            65508 MiB

Your MPI job will continue, but may be behave poorly and/or hang.
--------------------------------------------------------------------------
Processor 7: sum of squares up to 650000*650000 is 91541877916774992
Processor 8: sum of squares up to 650000*650000 is 91541877916774992
Processor 9: sum of squares up to 650000*650000 is 91541877916774992
Processor 10: sum of squares up to 650000*650000 is 91541877916774992
Processor 11: sum of squares up to 650000*650000 is 91541877916774992
Processor 12: sum of squares up to 650000*650000 is 91541877916774992
Processor 13: sum of squares up to 650000*650000 is 91541877916774992
Processor 14: sum of squares up to 650000*650000 is 91541877916774992
Processor 15: sum of squares up to 650000*650000 is 91541877916774992
Processor 0: sum of squares up to 650000*650000 is 91541877916774992
This took only 0.011717 seconds.
Processor 1: sum of squares up to 650000*650000 is 91541877916774992
Processor 2: sum of squares up to 650000*650000 is 91541877916774992
Processor 3: sum of squares up to 650000*650000 is 91541877916774992
Processor 4: sum of squares up to 650000*650000 is 91541877916774992
Processor 6: sum of squares up to 650000*650000 is 91541877916774992
Processor 5: sum of squares up to 650000*650000 is 91541877916774992
[node03:16028] 15 more processes have sent help message help-mpi-btl-openib.txt / reg mem limit low
[node03:16028] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Sender: LSF System <hpcadmin@node04>
Subject: Job 9456: </gpfs1/openmpi/bin/mpirun ./product 800000> Done

Job </gpfs1/openmpi/bin/mpirun ./product 800000> was submitted from host <head1> by user <kmd14> in cluster <head1_cluster1>.
Job was executed on host(s) <16*node04>, in queue <medium_priority>, as user <kmd14> in cluster <head1_cluster1>.
</gpfs1/kmd14> was used as the home directory.
</gpfs1/kmd14/297M/Assignments/Assignment 1/MPI> was used as the working directory.
Started at Mon Feb 24 16:31:01 2014
Results reported at Mon Feb 24 16:31:04 2014

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/gpfs1/openmpi/bin/mpirun ./product 800000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time   :      1.60 sec.
    Max Memory :         1 MB
    Max Swap   :        30 MB

    Max Processes  :         1
    Max Threads    :         1

The output (if any) follows:

--------------------------------------------------------------------------
WARNING: It appears that your OpenFabrics subsystem is configured to only
allow registering part of your physical memory.  This can cause MPI jobs to
run with erratic performance, hang, and/or crash.

This may be caused by your OpenFabrics vendor limiting the amount of
physical memory that can be registered.  You should investigate the
relevant Linux kernel module parameters that control how much physical
memory can be registered, and increase them to allow registering all
physical memory on your machine.

See this Open MPI FAQ item for more information on these Linux kernel module
parameters:

    http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages

  Local host:              node04
  Registerable memory:     32768 MiB
  Total memory:            65508 MiB

Your MPI job will continue, but may be behave poorly and/or hang.
--------------------------------------------------------------------------
Processor 7: sum of squares up to 800000*800000 is 170666986666778080
Processor 8: sum of squares up to 800000*800000 is 170666986666778080
Processor 9: sum of squares up to 800000*800000 is 170666986666778080
Processor 10: sum of squares up to 800000*800000 is 170666986666778080
Processor 11: sum of squares up to 800000*800000 is 170666986666778080
Processor 12: sum of squares up to 800000*800000 is 170666986666778080
Processor 13: sum of squares up to 800000*800000 is 170666986666778080
Processor 14: sum of squares up to 800000*800000 is 170666986666778080
Processor 15: sum of squares up to 800000*800000 is 170666986666778080
Processor 0: sum of squares up to 800000*800000 is 170666986666778080
This took only 0.003893 seconds.
Processor 1: sum of squares up to 800000*800000 is 170666986666778080
Processor 2: sum of squares up to 800000*800000 is 170666986666778080
Processor 3: sum of squares up to 800000*800000 is 170666986666778080
Processor 4: sum of squares up to 800000*800000 is 170666986666778080
Processor 5: sum of squares up to 800000*800000 is 170666986666778080
Processor 6: sum of squares up to 800000*800000 is 170666986666778080
[node04:03006] 15 more processes have sent help message help-mpi-btl-openib.txt / reg mem limit low
[node04:03006] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Sender: LSF System <hpcadmin@node16>
Subject: Job 9455: </gpfs1/openmpi/bin/mpirun ./product 750000> Done

Job </gpfs1/openmpi/bin/mpirun ./product 750000> was submitted from host <head1> by user <kmd14> in cluster <head1_cluster1>.
Job was executed on host(s) <16*node16>, in queue <medium_priority>, as user <kmd14> in cluster <head1_cluster1>.
</gpfs1/kmd14> was used as the home directory.
</gpfs1/kmd14/297M/Assignments/Assignment 1/MPI> was used as the working directory.
Started at Mon Feb 24 16:31:01 2014
Results reported at Mon Feb 24 16:31:04 2014

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/gpfs1/openmpi/bin/mpirun ./product 750000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time   :      1.54 sec.
    Max Memory :         1 MB
    Max Swap   :        30 MB

    Max Processes  :         1
    Max Threads    :         1

The output (if any) follows:

--------------------------------------------------------------------------
WARNING: It appears that your OpenFabrics subsystem is configured to only
allow registering part of your physical memory.  This can cause MPI jobs to
run with erratic performance, hang, and/or crash.

This may be caused by your OpenFabrics vendor limiting the amount of
physical memory that can be registered.  You should investigate the
relevant Linux kernel module parameters that control how much physical
memory can be registered, and increase them to allow registering all
physical memory on your machine.

See this Open MPI FAQ item for more information on these Linux kernel module
parameters:

    http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages

  Local host:              node16
  Registerable memory:     32768 MiB
  Total memory:            65508 MiB

Your MPI job will continue, but may be behave poorly and/or hang.
--------------------------------------------------------------------------
Processor 0: sum of squares up to 750000*750000 is 140625281250125008
This took only 0.001085 seconds.
Processor 1: sum of squares up to 750000*750000 is 140625281250125008
Processor 2: sum of squares up to 750000*750000 is 140625281250125008
Processor 3: sum of squares up to 750000*750000 is 140625281250125008
Processor 4: sum of squares up to 750000*750000 is 140625281250125008
Processor 5: sum of squares up to 750000*750000 is 140625281250125008
Processor 6: sum of squares up to 750000*750000 is 140625281250125008
Processor 7: sum of squares up to 750000*750000 is 140625281250125008
Processor 8: sum of squares up to 750000*750000 is 140625281250125008
Processor 9: sum of squares up to 750000*750000 is 140625281250125008
Processor 10: sum of squares up to 750000*750000 is 140625281250125008
Processor 11: sum of squares up to 750000*750000 is 140625281250125008
Processor 12: sum of squares up to 750000*750000 is 140625281250125008
Processor 13: sum of squares up to 750000*750000 is 140625281250125008
Processor 14: sum of squares up to 750000*750000 is 140625281250125008
Processor 15: sum of squares up to 750000*750000 is 140625281250125008
[node16:12989] 15 more processes have sent help message help-mpi-btl-openib.txt / reg mem limit low
[node16:12989] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Sender: LSF System <hpcadmin@node06>
Subject: Job 9454: </gpfs1/openmpi/bin/mpirun ./product 700000> Done

Job </gpfs1/openmpi/bin/mpirun ./product 700000> was submitted from host <head1> by user <kmd14> in cluster <head1_cluster1>.
Job was executed on host(s) <16*node06>, in queue <medium_priority>, as user <kmd14> in cluster <head1_cluster1>.
</gpfs1/kmd14> was used as the home directory.
</gpfs1/kmd14/297M/Assignments/Assignment 1/MPI> was used as the working directory.
Started at Mon Feb 24 16:31:01 2014
Results reported at Mon Feb 24 16:31:04 2014

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/gpfs1/openmpi/bin/mpirun ./product 700000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time   :      1.70 sec.
    Max Memory :         1 MB
    Max Swap   :        30 MB

    Max Processes  :         1
    Max Threads    :         1

The output (if any) follows:

--------------------------------------------------------------------------
WARNING: It appears that your OpenFabrics subsystem is configured to only
allow registering part of your physical memory.  This can cause MPI jobs to
run with erratic performance, hang, and/or crash.

This may be caused by your OpenFabrics vendor limiting the amount of
physical memory that can be registered.  You should investigate the
relevant Linux kernel module parameters that control how much physical
memory can be registered, and increase them to allow registering all
physical memory on your machine.

See this Open MPI FAQ item for more information on these Linux kernel module
parameters:

    http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages

  Local host:              node06
  Registerable memory:     32768 MiB
  Total memory:            65508 MiB

Your MPI job will continue, but may be behave poorly and/or hang.
--------------------------------------------------------------------------
Processor 10: sum of squares up to 700000*700000 is 114333578333450000
Processor 11: sum of squares up to 700000*700000 is 114333578333450000
Processor 12: sum of squares up to 700000*700000 is 114333578333450000
Processor 13: sum of squares up to 700000*700000 is 114333578333450000
Processor 14: sum of squares up to 700000*700000 is 114333578333450000
Processor 15: sum of squares up to 700000*700000 is 114333578333450000
Processor 0: sum of squares up to 700000*700000 is 114333578333450000
This took only 0.006349 seconds.
Processor 1: sum of squares up to 700000*700000 is 114333578333450000
Processor 2: sum of squares up to 700000*700000 is 114333578333450000
Processor 3: sum of squares up to 700000*700000 is 114333578333450000
Processor 4: sum of squares up to 700000*700000 is 114333578333450000
Processor 5: sum of squares up to 700000*700000 is 114333578333450000
Processor 6: sum of squares up to 700000*700000 is 114333578333450000
Processor 7: sum of squares up to 700000*700000 is 114333578333450000
Processor 8: sum of squares up to 700000*700000 is 114333578333450000
Processor 9: sum of squares up to 700000*700000 is 114333578333450000
[node06:28575] 15 more processes have sent help message help-mpi-btl-openib.txt / reg mem limit low
[node06:28575] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Sender: LSF System <hpcadmin@node13>
Subject: Job 9457: </gpfs1/openmpi/bin/mpirun ./product 850000> Done

Job </gpfs1/openmpi/bin/mpirun ./product 850000> was submitted from host <head1> by user <kmd14> in cluster <head1_cluster1>.
Job was executed on host(s) <16*node13>, in queue <medium_priority>, as user <kmd14> in cluster <head1_cluster1>.
</gpfs1/kmd14> was used as the home directory.
</gpfs1/kmd14/297M/Assignments/Assignment 1/MPI> was used as the working directory.
Started at Mon Feb 24 16:31:03 2014
Results reported at Mon Feb 24 16:31:05 2014

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/gpfs1/openmpi/bin/mpirun ./product 850000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time   :      1.82 sec.
    Max Memory :         1 MB
    Max Swap   :        30 MB

    Max Processes  :         1
    Max Threads    :         1

The output (if any) follows:

--------------------------------------------------------------------------
WARNING: It appears that your OpenFabrics subsystem is configured to only
allow registering part of your physical memory.  This can cause MPI jobs to
run with erratic performance, hang, and/or crash.

This may be caused by your OpenFabrics vendor limiting the amount of
physical memory that can be registered.  You should investigate the
relevant Linux kernel module parameters that control how much physical
memory can be registered, and increase them to allow registering all
physical memory on your machine.

See this Open MPI FAQ item for more information on these Linux kernel module
parameters:

    http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages

  Local host:              node13
  Registerable memory:     32768 MiB
  Total memory:            65508 MiB

Your MPI job will continue, but may be behave poorly and/or hang.
--------------------------------------------------------------------------
Processor 7: sum of squares up to 850000*850000 is 204708694583428064
Processor 8: sum of squares up to 850000*850000 is 204708694583428064
Processor 9: sum of squares up to 850000*850000 is 204708694583428064
Processor 10: sum of squares up to 850000*850000 is 204708694583428064
Processor 11: sum of squares up to 850000*850000 is 204708694583428064
Processor 12: sum of squares up to 850000*850000 is 204708694583428064
Processor 13: sum of squares up to 850000*850000 is 204708694583428064
Processor 14: sum of squares up to 850000*850000 is 204708694583428064
Processor 15: sum of squares up to 850000*850000 is 204708694583428064
Processor 1: sum of squares up to 850000*850000 is 204708694583428064
Processor 2: sum of squares up to 850000*850000 is 204708694583428064
Processor 3: sum of squares up to 850000*850000 is 204708694583428064
Processor 4: sum of squares up to 850000*850000 is 204708694583428064
Processor 5: sum of squares up to 850000*850000 is 204708694583428064
Processor 6: sum of squares up to 850000*850000 is 204708694583428064
Processor 0: sum of squares up to 850000*850000 is 204708694583428064
This took only 0.013996 seconds.
[node13:29097] 15 more processes have sent help message help-mpi-btl-openib.txt / reg mem limit low
[node13:29097] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Sender: LSF System <hpcadmin@node01>
Subject: Job 9458: </gpfs1/openmpi/bin/mpirun ./product 900000> Done

Job </gpfs1/openmpi/bin/mpirun ./product 900000> was submitted from host <head1> by user <kmd14> in cluster <head1_cluster1>.
Job was executed on host(s) <16*node01>, in queue <medium_priority>, as user <kmd14> in cluster <head1_cluster1>.
</gpfs1/kmd14> was used as the home directory.
</gpfs1/kmd14/297M/Assignments/Assignment 1/MPI> was used as the working directory.
Started at Mon Feb 24 16:31:03 2014
Results reported at Mon Feb 24 16:31:05 2014

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/gpfs1/openmpi/bin/mpirun ./product 900000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time   :      1.76 sec.
    Max Memory :         1 MB
    Max Swap   :        30 MB

    Max Processes  :         1
    Max Threads    :         1

The output (if any) follows:

--------------------------------------------------------------------------
WARNING: It appears that your OpenFabrics subsystem is configured to only
allow registering part of your physical memory.  This can cause MPI jobs to
run with erratic performance, hang, and/or crash.

This may be caused by your OpenFabrics vendor limiting the amount of
physical memory that can be registered.  You should investigate the
relevant Linux kernel module parameters that control how much physical
memory can be registered, and increase them to allow registering all
physical memory on your machine.

See this Open MPI FAQ item for more information on these Linux kernel module
parameters:

    http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages

  Local host:              node01
  Registerable memory:     32768 MiB
  Total memory:            65508 MiB

Your MPI job will continue, but may be behave poorly and/or hang.
--------------------------------------------------------------------------
Processor 0: sum of squares up to 900000*900000 is 243000405000078080
This took only 0.007948 seconds.
Processor 1: sum of squares up to 900000*900000 is 243000405000078080
Processor 2: sum of squares up to 900000*900000 is 243000405000078080
Processor 3: sum of squares up to 900000*900000 is 243000405000078080
Processor 4: sum of squares up to 900000*900000 is 243000405000078080
Processor 5: sum of squares up to 900000*900000 is 243000405000078080
Processor 6: sum of squares up to 900000*900000 is 243000405000078080
Processor 7: sum of squares up to 900000*900000 is 243000405000078080
Processor 8: sum of squares up to 900000*900000 is 243000405000078080
Processor 9: sum of squares up to 900000*900000 is 243000405000078080
Processor 10: sum of squares up to 900000*900000 is 243000405000078080
Processor 11: sum of squares up to 900000*900000 is 243000405000078080
Processor 12: sum of squares up to 900000*900000 is 243000405000078080
Processor 13: sum of squares up to 900000*900000 is 243000405000078080
Processor 14: sum of squares up to 900000*900000 is 243000405000078080
Processor 15: sum of squares up to 900000*900000 is 243000405000078080
[node01:22994] 15 more processes have sent help message help-mpi-btl-openib.txt / reg mem limit low
[node01:22994] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Sender: LSF System <hpcadmin@node03>
Subject: Job 9459: </gpfs1/openmpi/bin/mpirun ./product 950000> Done

Job </gpfs1/openmpi/bin/mpirun ./product 950000> was submitted from host <head1> by user <kmd14> in cluster <head1_cluster1>.
Job was executed on host(s) <16*node03>, in queue <medium_priority>, as user <kmd14> in cluster <head1_cluster1>.
</gpfs1/kmd14> was used as the home directory.
</gpfs1/kmd14/297M/Assignments/Assignment 1/MPI> was used as the working directory.
Started at Mon Feb 24 16:31:04 2014
Results reported at Mon Feb 24 16:31:05 2014

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/gpfs1/openmpi/bin/mpirun ./product 950000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time   :      1.77 sec.
    Max Memory :         1 MB
    Max Swap   :        30 MB

    Max Processes  :         1
    Max Threads    :         1

The output (if any) follows:

--------------------------------------------------------------------------
WARNING: It appears that your OpenFabrics subsystem is configured to only
allow registering part of your physical memory.  This can cause MPI jobs to
run with erratic performance, hang, and/or crash.

This may be caused by your OpenFabrics vendor limiting the amount of
physical memory that can be registered.  You should investigate the
relevant Linux kernel module parameters that control how much physical
memory can be registered, and increase them to allow registering all
physical memory on your machine.

See this Open MPI FAQ item for more information on these Linux kernel module
parameters:

    http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages

  Local host:              node03
  Registerable memory:     32768 MiB
  Total memory:            65508 MiB

Your MPI job will continue, but may be behave poorly and/or hang.
--------------------------------------------------------------------------
Processor 10: sum of squares up to 950000*950000 is 285792117916728096
Processor 11: sum of squares up to 950000*950000 is 285792117916728096
Processor 12: sum of squares up to 950000*950000 is 285792117916728096
Processor 13: sum of squares up to 950000*950000 is 285792117916728096
Processor 14: sum of squares up to 950000*950000 is 285792117916728096
Processor 15: sum of squares up to 950000*950000 is 285792117916728096
Processor 0: sum of squares up to 950000*950000 is 285792117916728096
This took only 0.005653 seconds.
Processor 1: sum of squares up to 950000*950000 is 285792117916728096
Processor 2: sum of squares up to 950000*950000 is 285792117916728096
Processor 3: sum of squares up to 950000*950000 is 285792117916728096
Processor 4: sum of squares up to 950000*950000 is 285792117916728096
Processor 5: sum of squares up to 950000*950000 is 285792117916728096
Processor 6: sum of squares up to 950000*950000 is 285792117916728096
Processor 7: sum of squares up to 950000*950000 is 285792117916728096
Processor 8: sum of squares up to 950000*950000 is 285792117916728096
Processor 9: sum of squares up to 950000*950000 is 285792117916728096
[node03:16084] 15 more processes have sent help message help-mpi-btl-openib.txt / reg mem limit low
[node03:16084] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Sender: LSF System <hpcadmin@node06>
Subject: Job 9460: </gpfs1/openmpi/bin/mpirun ./product 1000000> Done

Job </gpfs1/openmpi/bin/mpirun ./product 1000000> was submitted from host <head1> by user <kmd14> in cluster <head1_cluster1>.
Job was executed on host(s) <16*node06>, in queue <medium_priority>, as user <kmd14> in cluster <head1_cluster1>.
</gpfs1/kmd14> was used as the home directory.
</gpfs1/kmd14/297M/Assignments/Assignment 1/MPI> was used as the working directory.
Started at Mon Feb 24 16:31:04 2014
Results reported at Mon Feb 24 16:31:06 2014

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/gpfs1/openmpi/bin/mpirun ./product 1000000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time   :      1.67 sec.
    Max Memory :         1 MB
    Max Swap   :        30 MB

    Max Processes  :         1
    Max Threads    :         1

The output (if any) follows:

--------------------------------------------------------------------------
WARNING: It appears that your OpenFabrics subsystem is configured to only
allow registering part of your physical memory.  This can cause MPI jobs to
run with erratic performance, hang, and/or crash.

This may be caused by your OpenFabrics vendor limiting the amount of
physical memory that can be registered.  You should investigate the
relevant Linux kernel module parameters that control how much physical
memory can be registered, and increase them to allow registering all
physical memory on your machine.

See this Open MPI FAQ item for more information on these Linux kernel module
parameters:

    http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages

  Local host:              node06
  Registerable memory:     32768 MiB
  Total memory:            65508 MiB

Your MPI job will continue, but may be behave poorly and/or hang.
--------------------------------------------------------------------------
Processor 10: sum of squares up to 1000000*1000000 is 333333833333378112
Processor 11: sum of squares up to 1000000*1000000 is 333333833333378112
Processor 12: sum of squares up to 1000000*1000000 is 333333833333378112
Processor 13: sum of squares up to 1000000*1000000 is 333333833333378112
Processor 14: sum of squares up to 1000000*1000000 is 333333833333378112
Processor 15: sum of squares up to 1000000*1000000 is 333333833333378112
Processor 0: sum of squares up to 1000000*1000000 is 333333833333378112
This took only 0.002118 seconds.
Processor 1: sum of squares up to 1000000*1000000 is 333333833333378112
Processor 2: sum of squares up to 1000000*1000000 is 333333833333378112
Processor 3: sum of squares up to 1000000*1000000 is 333333833333378112
Processor 4: sum of squares up to 1000000*1000000 is 333333833333378112
Processor 5: sum of squares up to 1000000*1000000 is 333333833333378112
Processor 6: sum of squares up to 1000000*1000000 is 333333833333378112
Processor 7: sum of squares up to 1000000*1000000 is 333333833333378112
Processor 8: sum of squares up to 1000000*1000000 is 333333833333378112
Processor 9: sum of squares up to 1000000*1000000 is 333333833333378112
[node06:28629] 15 more processes have sent help message help-mpi-btl-openib.txt / reg mem limit low
[node06:28629] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
